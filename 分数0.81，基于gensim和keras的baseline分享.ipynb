{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码参考了https://www.jianshu.com/p/fba7df3a76fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这是基于gensim和keras的简单basline，直接对文本进行情感分类，并且没有对实体进行筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Train_Data.csv', 'Test_Data.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Bidirectional, LSTM, Dense, Embedding, Dropout, Activation, Softmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "import pickle as pkl\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "from conf import conf\n",
    "import gensim\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "DATA_DIR = '****'#将此处改为自己的数据存放路径\n",
    "print(os.listdir(DATA_DIR))\n",
    "TrainDataPath = join(DATA_DIR, 'Train_Data.csv')\n",
    "TestDataPath = join(DATA_DIR, 'Test_Data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取文本用于gensim训练词向量\n",
    "def read_data(data_path):\n",
    "    dataframe = pd.read_csv(data_path)[['text']]\n",
    "    textList = dataframe['text'].to_list()\n",
    "    return textList\n",
    "\n",
    "#训练gensim词向量模型\n",
    "def train_word2vec(sentences, save_path):\n",
    "    sentences_seg = []\n",
    "    sen_str = \"\\n\".join(sentences)\n",
    "    res = jieba.lcut(sen_str)\n",
    "    seg_str = \" \".join(res)\n",
    "    sen_list = seg_str.split(\"\\n\")\n",
    "    for i in sen_list:\n",
    "        sentences_seg.append(i.split())\n",
    "    print(\"开始训练词向量\")\n",
    "    #     logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    model = Word2Vec(sentences_seg,\n",
    "                     size=100,  # 词向量维度\n",
    "                     min_count=2,  # 词频阈值\n",
    "                     window=5)  # 窗口大小\n",
    "    model.save(save_path)\n",
    "    return model\n",
    "\n",
    "#将gensim模型转换为单词到id的映射和词向量矩阵\n",
    "def generate_id2wec(word2vec_model):\n",
    "    gensim_dict = Dictionary()\n",
    "    gensim_dict.doc2bow(model.wv.vocab.keys(), allow_update=True)\n",
    "    w2id = {v: k + 1 for k, v in gensim_dict.items()}  # 词语的索引，从1开始编号\n",
    "    w2vec = {word: model[word] for word in w2id.keys()}  # 词语的词向量\n",
    "    n_vocabs = len(w2id) + 1\n",
    "    embedding_weights = np.zeros((n_vocabs, 100))\n",
    "    for w, index in w2id.items():  # 从索引为1的词语开始，用词向量填充矩阵\n",
    "        embedding_weights[index, :] = w2vec[w]\n",
    "    return w2id, embedding_weights\n",
    "\n",
    "# 单词转索引数字\n",
    "def text_to_array(w2index, senlist):  \n",
    "    sentences_array = []\n",
    "    for sen in senlist:\n",
    "        new_sen = [w2index.get(word, 0) for word in sen]  \n",
    "        sentences_array.append(new_sen)\n",
    "    return np.array(sentences_array)\n",
    "\n",
    "#准备用于训练和验证的数据\n",
    "def prepare_data(w2id, max_len=200):\n",
    "    df = pd.read_csv(TrainDataPath)[['text', 'negative']]\n",
    "    sentences = df['text'].to_list()\n",
    "    labels = df['negative'].to_list()\n",
    "    X_train, X_val, y_train, y_val = train_test_split(sentences, labels, test_size=0.2)\n",
    "    X_train = text_to_array(w2id, X_train)\n",
    "    X_val = text_to_array(w2id, X_val)\n",
    "    X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "    X_val = pad_sequences(X_val, maxlen=max_len)\n",
    "    return np.array(X_train), np_utils.to_categorical(y_train), np.array(X_val), np_utils.to_categorical(y_val)\n",
    "\n",
    "#准备用于测试的数据\n",
    "def prepare_predict_data(w2id, max_len=200):\n",
    "    df = pd.read_csv(TestDataPath)[['text', 'entity']]\n",
    "    sentences = df['text'].to_list()\n",
    "    sentences = [str(s) for s in sentences]\n",
    "    entities = df['entity'].to_list()\n",
    "    return sentences, entities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义模型\n",
    "class Sentiment:\n",
    "    def __init__(self, w2id, embedding_weights, Embedding_dim, maxlen, labels_category):\n",
    "        self.Embedding_dim = Embedding_dim\n",
    "        self.embedding_weights = embedding_weights\n",
    "        self.vocab = w2id\n",
    "        self.labels_category = labels_category\n",
    "        self.maxlen = maxlen\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        # input dim(140,100)\n",
    "        model.add(Embedding(output_dim=self.Embedding_dim,\n",
    "                            input_dim=len(self.vocab) + 1,\n",
    "                            weights=[self.embedding_weights],\n",
    "                            input_length=self.maxlen))\n",
    "        model.add(Bidirectional(LSTM(50), merge_mode='concat'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(self.labels_category))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    def train(self, X_train, y_train, X_test, y_test, batch_size = 128, n_epoch=35):\n",
    "        early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto',\n",
    "                                                   baseline=None, restore_best_weights=False)\n",
    "\n",
    "        self.model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epoch,\n",
    "                       validation_data=(X_test, y_test), callbacks=[early_stop])\n",
    "        self.model.save('sentiment.h5')\n",
    "\n",
    "    def predict(self, model_path, sents):\n",
    "        model = self.model\n",
    "        model.load_weights(model_path)\n",
    "        new_sen_lists = [jieba.lcut(new_sen) for new_sen in sents]\n",
    "        sen2id = [[self.vocab.get(word, 0) for word in new_sen_list] for new_sen_list in new_sen_lists]\n",
    "        sen_input = pad_sequences(sen2id, maxlen=self.maxlen)\n",
    "        res = model.predict(sen_input)\n",
    "        return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练词向量\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/njuciairs/anaconda3/envs/tftorch/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/home/njuciairs/anaconda3/envs/tftorch/lib/python3.6/site-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 100)          2511000   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 202       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,571,602\n",
      "Trainable params: 2,571,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3999 samples, validate on 1000 samples\n",
      "Epoch 1/35\n",
      "3999/3999 [==============================] - 33s 8ms/step - loss: 0.4221 - acc: 0.7857 - val_loss: 0.2997 - val_acc: 0.8740\n",
      "Epoch 2/35\n",
      "3999/3999 [==============================] - 32s 8ms/step - loss: 0.2604 - acc: 0.8840 - val_loss: 0.2437 - val_acc: 0.8920\n",
      "Epoch 3/35\n",
      "3999/3999 [==============================] - 32s 8ms/step - loss: 0.2102 - acc: 0.9120 - val_loss: 0.2211 - val_acc: 0.9010\n",
      "Epoch 4/35\n",
      "3999/3999 [==============================] - 32s 8ms/step - loss: 0.1701 - acc: 0.9312 - val_loss: 0.2099 - val_acc: 0.9150\n",
      "Epoch 5/35\n",
      "3999/3999 [==============================] - 32s 8ms/step - loss: 0.1381 - acc: 0.9462 - val_loss: 0.2211 - val_acc: 0.9110\n",
      "Epoch 6/35\n",
      "3999/3999 [==============================] - 32s 8ms/step - loss: 0.1007 - acc: 0.9625 - val_loss: 0.2326 - val_acc: 0.9210\n",
      "Epoch 7/35\n",
      "3999/3999 [==============================] - 32s 8ms/step - loss: 0.0864 - acc: 0.9667 - val_loss: 0.2463 - val_acc: 0.9120\n",
      "Epoch 8/35\n",
      "3999/3999 [==============================] - 32s 8ms/step - loss: 0.0555 - acc: 0.9830 - val_loss: 0.2396 - val_acc: 0.9150\n",
      "Epoch 9/35\n",
      "3999/3999 [==============================] - 32s 8ms/step - loss: 0.0502 - acc: 0.9850 - val_loss: 0.2230 - val_acc: 0.9280\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#训练词向量\n",
    "texts1 = read_data(TrainDataPath)\n",
    "texts2 = read_data(TestDataPath)\n",
    "texts = texts1 + texts2\n",
    "texts = [str(t) for t in texts]\n",
    "model = train_word2vec(texts, 'word2vec.model')\n",
    "\n",
    "w2id, embedding_weights = generate_id2wec(model)\n",
    "x_train, y_trian, x_val, y_val = prepare_data(w2id, 100)\n",
    "\n",
    "#训练keras文本分类模型\n",
    "senti = Sentiment(w2id, embedding_weights, 100, 100, 2)\n",
    "senti.train(x_train, y_trian, x_val, y_val, 30)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用训练好的模型进行预测\n",
    "X_test, entities = prepare_predict_data(w2id, 100)\n",
    "rs = senti.predict('sentiment.h5', X_test)\n",
    "\n",
    "# 提交答案\n",
    "df = pd.read_csv(TestDataPath)[['id', 'entity']]\n",
    "df['negative'] = np.argmax(rs, axis=1)\n",
    "\n",
    "df.columns = ['id', 'key_entity', 'negative']\n",
    "df = df[['id', 'negative', 'key_entity']]\n",
    "df.loc[df['negative']==0,'key_entity'] = np.nan\n",
    "df.to_csv('basic_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>negative</th>\n",
       "      <th>key_entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>f3b61b38</td>\n",
       "      <td>1</td>\n",
       "      <td>小资钱包;资易贷</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>84b12bae</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6abf4a82</td>\n",
       "      <td>1</td>\n",
       "      <td>嘉石榴</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8d076785</td>\n",
       "      <td>1</td>\n",
       "      <td>宜贷网(沪);易捷金融;宜贷网</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>d65a1577</td>\n",
       "      <td>1</td>\n",
       "      <td>贵金属</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  negative       key_entity\n",
       "0  f3b61b38         1         小资钱包;资易贷\n",
       "1  84b12bae         0              NaN\n",
       "2  6abf4a82         1              嘉石榴\n",
       "3  8d076785         1  宜贷网(沪);易捷金融;宜贷网\n",
       "4  d65a1577         1              贵金属"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查看结果\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
